{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6307d599",
   "metadata": {},
   "source": [
    "Notes: \n",
    "* Answer each question in a separate Jupynet Notebook Cell\n",
    "* Pleas keep the code in your cells short. \n",
    "  * In notebook programming cells are typicaly short to facilitate reading. \n",
    "  * If well toughout, most answers in this assignment won're require more than 3 or 4 lines of code. \n",
    "* Do no change the list of import, i.e., do not add additional libraries. Those included are the only ones you are allowed to use.\n",
    "* Add your first and Last name below:\n",
    "\n",
    "Jun Miao\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a78e91d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from itertools import product\n",
    "from collections import Counter\n",
    "from tqdm.notebook import tqdm\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1258957f",
   "metadata": {},
   "source": [
    "In this assignment you will be working with Corona Virus (SARS-CoV2) data that was obtained from the [National Center for Biotechnology Information](https://www.ncbi.nlm.nih.gov/). You will need two files. The first (`data/coronavirus_info.csv`) is small and is provided in the GitHub Repo. The second  (`data_report.jsonl`) is larger so you will need to download a compressed version, which you will need to uncompress prior to using. You can downlod the second file here:\n",
    "\n",
    "https://www.dropbox.com/s/qdn67rshygz06ff/data_report.jsonl.gz?dl=0\n",
    "\n",
    "We start by loading `data/coronavirus_info.csv` (Code provided below)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b03484ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Nucleotide Accession</th>\n",
       "      <th>Species Name</th>\n",
       "      <th>Virus Genus</th>\n",
       "      <th>Virus Family</th>\n",
       "      <th>Isolate Name</th>\n",
       "      <th>Nucleotide Length</th>\n",
       "      <th>Geo Location</th>\n",
       "      <th>Collection Date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NC_045512.2</td>\n",
       "      <td>Severe acute respiratory syndrome coronavirus 2</td>\n",
       "      <td>Betacoronavirus</td>\n",
       "      <td>Coronaviridae</td>\n",
       "      <td>Wuhan-Hu-1</td>\n",
       "      <td>29903</td>\n",
       "      <td>Asia; China</td>\n",
       "      <td>2019-12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>OK058807.1</td>\n",
       "      <td>Severe acute respiratory syndrome coronavirus 2</td>\n",
       "      <td>Betacoronavirus</td>\n",
       "      <td>Coronaviridae</td>\n",
       "      <td>SARS-CoV-2/human/USA/MA-MASPHL-04825/2021</td>\n",
       "      <td>29801</td>\n",
       "      <td>North America; USA</td>\n",
       "      <td>2021-07-29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>OK058777.1</td>\n",
       "      <td>Severe acute respiratory syndrome coronavirus 2</td>\n",
       "      <td>Betacoronavirus</td>\n",
       "      <td>Coronaviridae</td>\n",
       "      <td>SARS-CoV-2/human/USA/MA-MASPHL-04790/2021</td>\n",
       "      <td>29771</td>\n",
       "      <td>North America; USA</td>\n",
       "      <td>2021-08-10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>OK058695.1</td>\n",
       "      <td>Severe acute respiratory syndrome coronavirus 2</td>\n",
       "      <td>Betacoronavirus</td>\n",
       "      <td>Coronaviridae</td>\n",
       "      <td>SARS-CoV-2/human/USA/MA-MASPHL-04700/2021</td>\n",
       "      <td>29820</td>\n",
       "      <td>North America; USA</td>\n",
       "      <td>2021-08-15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>OK058662.1</td>\n",
       "      <td>Severe acute respiratory syndrome coronavirus 2</td>\n",
       "      <td>Betacoronavirus</td>\n",
       "      <td>Coronaviridae</td>\n",
       "      <td>SARS-CoV-2/human/USA/MA-MASPHL-04651/2021</td>\n",
       "      <td>29798</td>\n",
       "      <td>North America; USA</td>\n",
       "      <td>2021-08-09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>OK058592.1</td>\n",
       "      <td>Severe acute respiratory syndrome coronavirus 2</td>\n",
       "      <td>Betacoronavirus</td>\n",
       "      <td>Coronaviridae</td>\n",
       "      <td>SARS-CoV-2/human/USA/MA-MASPHL-04499/2021</td>\n",
       "      <td>29802</td>\n",
       "      <td>North America; USA</td>\n",
       "      <td>2021-07-26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>OK056996.1</td>\n",
       "      <td>Severe acute respiratory syndrome coronavirus 2</td>\n",
       "      <td>Betacoronavirus</td>\n",
       "      <td>Coronaviridae</td>\n",
       "      <td>SARS-CoV-2/human/USA/FL-CDC-QDX27934346/2021</td>\n",
       "      <td>29775</td>\n",
       "      <td>North America; USA: Florida</td>\n",
       "      <td>2021-08-18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>OK056909.1</td>\n",
       "      <td>Severe acute respiratory syndrome coronavirus 2</td>\n",
       "      <td>Betacoronavirus</td>\n",
       "      <td>Coronaviridae</td>\n",
       "      <td>SARS-CoV-2/human/USA/CA-CDC-QDX27909662/2021</td>\n",
       "      <td>29775</td>\n",
       "      <td>North America; USA: California</td>\n",
       "      <td>2021-08-16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>OK056850.1</td>\n",
       "      <td>Severe acute respiratory syndrome coronavirus 2</td>\n",
       "      <td>Betacoronavirus</td>\n",
       "      <td>Coronaviridae</td>\n",
       "      <td>SARS-CoV-2/human/USA/FL-CDC-QDX27934406/2021</td>\n",
       "      <td>29763</td>\n",
       "      <td>North America; USA: Florida</td>\n",
       "      <td>2021-08-18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>OK056784.1</td>\n",
       "      <td>Severe acute respiratory syndrome coronavirus 2</td>\n",
       "      <td>Betacoronavirus</td>\n",
       "      <td>Coronaviridae</td>\n",
       "      <td>SARS-CoV-2/human/USA/NY-CDC-QDX28007789/2021</td>\n",
       "      <td>29775</td>\n",
       "      <td>North America; USA: New York</td>\n",
       "      <td>2021-08-21</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Nucleotide Accession                                     Species Name  \\\n",
       "0          NC_045512.2  Severe acute respiratory syndrome coronavirus 2   \n",
       "1           OK058807.1  Severe acute respiratory syndrome coronavirus 2   \n",
       "2           OK058777.1  Severe acute respiratory syndrome coronavirus 2   \n",
       "3           OK058695.1  Severe acute respiratory syndrome coronavirus 2   \n",
       "4           OK058662.1  Severe acute respiratory syndrome coronavirus 2   \n",
       "5           OK058592.1  Severe acute respiratory syndrome coronavirus 2   \n",
       "6           OK056996.1  Severe acute respiratory syndrome coronavirus 2   \n",
       "7           OK056909.1  Severe acute respiratory syndrome coronavirus 2   \n",
       "8           OK056850.1  Severe acute respiratory syndrome coronavirus 2   \n",
       "9           OK056784.1  Severe acute respiratory syndrome coronavirus 2   \n",
       "\n",
       "       Virus Genus   Virus Family  \\\n",
       "0  Betacoronavirus  Coronaviridae   \n",
       "1  Betacoronavirus  Coronaviridae   \n",
       "2  Betacoronavirus  Coronaviridae   \n",
       "3  Betacoronavirus  Coronaviridae   \n",
       "4  Betacoronavirus  Coronaviridae   \n",
       "5  Betacoronavirus  Coronaviridae   \n",
       "6  Betacoronavirus  Coronaviridae   \n",
       "7  Betacoronavirus  Coronaviridae   \n",
       "8  Betacoronavirus  Coronaviridae   \n",
       "9  Betacoronavirus  Coronaviridae   \n",
       "\n",
       "                                   Isolate Name  Nucleotide Length  \\\n",
       "0                                    Wuhan-Hu-1              29903   \n",
       "1     SARS-CoV-2/human/USA/MA-MASPHL-04825/2021              29801   \n",
       "2     SARS-CoV-2/human/USA/MA-MASPHL-04790/2021              29771   \n",
       "3     SARS-CoV-2/human/USA/MA-MASPHL-04700/2021              29820   \n",
       "4     SARS-CoV-2/human/USA/MA-MASPHL-04651/2021              29798   \n",
       "5     SARS-CoV-2/human/USA/MA-MASPHL-04499/2021              29802   \n",
       "6  SARS-CoV-2/human/USA/FL-CDC-QDX27934346/2021              29775   \n",
       "7  SARS-CoV-2/human/USA/CA-CDC-QDX27909662/2021              29775   \n",
       "8  SARS-CoV-2/human/USA/FL-CDC-QDX27934406/2021              29763   \n",
       "9  SARS-CoV-2/human/USA/NY-CDC-QDX28007789/2021              29775   \n",
       "\n",
       "                     Geo Location Collection Date  \n",
       "0                     Asia; China         2019-12  \n",
       "1              North America; USA      2021-07-29  \n",
       "2              North America; USA      2021-08-10  \n",
       "3              North America; USA      2021-08-15  \n",
       "4              North America; USA      2021-08-09  \n",
       "5              North America; USA      2021-07-26  \n",
       "6     North America; USA: Florida      2021-08-18  \n",
       "7  North America; USA: California      2021-08-16  \n",
       "8     North America; USA: Florida      2021-08-18  \n",
       "9    North America; USA: New York      2021-08-21  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# CODE PROVIDED -- DO NOT REMOVE\n",
    "table = pd.read_csv(\"data/coronavirus_info.csv\", low_memory=False)\n",
    "table = table.drop([\"US State\", \"Host Name\", \"Host Taxonomy ID\", \"Sequence Type\", \"Species Taxonomy Id\", \"Nuc Completeness\", \"BioProject\", \"BioSample\"], axis=1)\n",
    "\n",
    "missing = table[\"Geo Location\"].isnull()\n",
    "table.loc[missing, \"Geo Location\"] = \"\"\n",
    "\n",
    "\n",
    "table.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "693ce7f2",
   "metadata": {},
   "source": [
    "### Q.1\n",
    "\n",
    "* The location of each of the sequences is recorded under the `Geo Location` column.  How many entries are from Asia?\n",
    "  * Note that for some records, the `Geo Location` column is missing\n",
    "  * Display the results using the following format: \n",
    "    Asia: XXXX,\n",
    "    North America': XXXX,\n",
    "    Europe: XXXX,\n",
    "    Oceania: XXXX,\n",
    "    Africa: XXXX,\n",
    "    South America: XXXX \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fedd7a41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Asia: 3903, North America: 224014, Europe: 189100, Oceania: 10301, Africa: 1405, South America: 534\n"
     ]
    }
   ],
   "source": [
    "result_dict = {'Asia': 0, 'North America': 0, 'Europe': 0, 'Oceania': 0, 'Africa': 0, 'South America': 0}\n",
    "\n",
    "def region_count(i):\n",
    "    for k in result_dict.keys():\n",
    "        if k in i:\n",
    "            result_dict[k] += 1\n",
    "\n",
    "table['Geo Location'].apply(region_count)\n",
    "print(f\"Asia: {result_dict['Asia']}, North America: {result_dict['North America']}, Europe: {result_dict['Europe']}, \"\n",
    "      f\"Oceania: {result_dict['Oceania']}, Africa: {result_dict['Africa']}, South America: {result_dict['South America']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f902c32",
   "metadata": {},
   "source": [
    "### Q.2\n",
    "Use the `coronavirus_info.csv` table to count the entries that are from Hawaii. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "af84040b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "119"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def filter_hawaii(x):\n",
    "    return 1 if 'Hawaii' in x else 0\n",
    "\n",
    "hawaii_region = table['Geo Location'].apply(filter_hawaii)\n",
    "hawaii_region.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10f25278",
   "metadata": {},
   "source": [
    "### Q.3\n",
    "\n",
    "The file `data_report.jsonl` contains the variants of the virus in the DB. This `json` file is a list of records (one per line) for each one of the genomes in the database. Before we work with the large file, we will experiment with a file containing a single record.\n",
    "\n",
    "The file `single_record.json` contains a single sample record. Use the `JSON library to load the file `single_record.json` into a variable called `sample_vir_record`\n",
    "\n",
    "1. how many first-level keys does this record have?\n",
    "  * Do not count nested keys. Only those at the top level should be counted\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ff53f4d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "sample_vir_record = json.load(open('./data/single_record.json', 'r'))\n",
    "len(sample_vir_record.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9679620a",
   "metadata": {},
   "source": [
    "### Q.4\n",
    "\n",
    "Each Covid various in this database is classified according to a system referred to as the Pangolin (Phylogenetic Assignment of Named Global Outbreak LINeages) classification. It is not essential to complete the assignment that you understand this system, but if you're interested in learning more, see:\n",
    "\n",
    "https://cov-lineages.org/resources/pangolin.html\n",
    "\n",
    "The Pangolin classification of this sample record is nested within the `virus` key:\n",
    "```json\n",
    "{ ...\n",
    "  \"virus\": {\n",
    "              ...\n",
    "              \"pangolinClassification\": \n",
    "              ...\n",
    "            }\n",
    "  ... \n",
    "}\n",
    "```\n",
    "Write code to extract the classification of this record. The result should be `B.1.1.214`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a0084d08",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'B.1.1.214'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_vir_record['virus']['pangolinClassification']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7db4f1ce",
   "metadata": {},
   "source": [
    "We hear in the news about `Alpha`, `Beta`, `Delta` variants of concern and recently the Mu variant as being of interest. Basedon your answer to `Q.3`, you may have been tempted to infer that this virus is of type `Beta` since the first letter is `B`. In fact, this variant of type `Alpha` and is a variant of concern. Although not relevant to this exercise, the rules for naming new variants and the list of known `SARS-CoV-2` are provided here:\n",
    "\n",
    "https://www.pango.network/how-does-the-system-work/what-are-pango-lineages/\n",
    "\n",
    "https://cov-lineages.org/lineage_list.html\n",
    "\n",
    "\n",
    "The following short video is very helpful for understanding what a variant is, how it arises, how it's named, and why some variants are more concerning than others.\n",
    "\n",
    "https://www.youtube.com/watch?v=B8UEZ9cfgz4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47a61d70",
   "metadata": {},
   "source": [
    "### Q.5\n",
    "\n",
    "Write Python code that counts all the different kinds of variants in `data_report.jsonl`. \n",
    "Because the file is 7GB, you won't likely be able to load it into your laptop's RAM using Python. I encountered this error when trying to open it on my laptop\n",
    "\n",
    "![](https://www.dropbox.com/s/lieo685pafkgm5e/ram_error.png?dl=1)\n",
    "\n",
    "\n",
    "It would be easy to extract the data from the pangolinClassification field of each `json` record by reading each line (a record) at a time.\n",
    "\n",
    "The list of current variants of concern we are interested in are:\n",
    "      * Alpha (B.1.1.7)\n",
    "      * Beta (B.1.351, B.1.351.2, B.1.351.3)\n",
    "      * Delta (B.1.617.2, AY.1, AY.2, AY.3)\n",
    "      * Gamma (P.1, P.1.1, P.1.2) \n",
    "\n",
    "You should get something similar to what follows:\n",
    "```\n",
    "Alpha: X\n",
    "Beta: X\n",
    "Delta: X\n",
    "Gamma: X\n",
    "```\n",
    "Where `X` represents the counts for relevant variants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "02544c57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alpha: 178423\n",
      "Beta: 584\n",
      "Delta: 4392\n",
      "Gamma: 6632\n"
     ]
    }
   ],
   "source": [
    "# Takes a while\n",
    "data_report = {}\n",
    "\n",
    "def variant_check(x):\n",
    "    try:\n",
    "        loaded = json.loads(x)\n",
    "        return loaded['virus']['pangolinClassification']\n",
    "    except KeyError:\n",
    "        return\n",
    "\n",
    "with open('./data/data_report.jsonl', 'r') as opened:\n",
    "    for line in opened:\n",
    "        temp = variant_check(line)\n",
    "        if temp:\n",
    "            try:\n",
    "                data_report[temp] += 1\n",
    "            except KeyError:\n",
    "                data_report[temp] = 1\n",
    "\n",
    "print(f\"Alpha: {data_report['B.1.1.7']}\")\n",
    "print(f\"Beta: {sum((data_report['B.1.351'], data_report['B.1.351.2'], data_report['B.1.351.3']))}\")\n",
    "print(f\"Delta: {sum((data_report['B.1.617.2'], data_report['AY.1'], data_report['AY.2'], data_report['AY.3']))}\")\n",
    "print(f\"Gamma: {sum((data_report['P.1'], data_report['P.1.1'], data_report['P.1.2']))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b18193ad",
   "metadata": {},
   "source": [
    "# Find similar viruses.\n",
    "\n",
    "It's often useful to compare viruses to study how similar strains are. While sophisticated algorithms to compare a pair of viruses exist, these are typically computationally intensive and cannot be used to carry out a large number of comparisons. \n",
    "\n",
    "An alternative, albeit less sensitive, approach consists of comparing word counts (called k-mers, where k is the word size) across genomes.  Suppose we have two viruses X and Y, with the following Genomes.\n",
    "```\n",
    "X = \"ACGTAGTGCATGTGTAGCTGTGTAGCTGTAC\"\n",
    "Y = \"ACTAGTGCATGTGTAGCTCTGTAGCTGATAC\"\n",
    "```\n",
    "\n",
    "To compare `X` and `Y`, we first vectorize these genomes by marking the presence of words (k-mers) as a boolean value, 0 if absent and 1 if the word is present. This method assumes that similar genomes will have the same words, which makes sense.\n",
    "\n",
    "This idea, which is referred to as the bag of words model is computationally efficient, making it ideal to vectorize text in big data analytics. Another variant of this model requires replacing the presence and absence by counts for each word.\n",
    "\n",
    "The code below vectorizes an input DNA sequence intro k-mers of size k=2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "210cc01e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CODE PROVIDED -- DO NOT REMOVE\n",
    "def get_kmer_2(X):\n",
    "    DNA = [\"A\", \"C\", \"G\", \"T\"]\n",
    "    words_size_2 = [\"\".join(dna_prod) for dna_prod in product(DNA, DNA)]\n",
    "    counts = pd.Series([0 for _ in words_size_2], index = words_size_2)\n",
    "    words_in_X = set([X[i:i+2] for i in range(0, len(X)-1)])\n",
    "    counts[list(words_in_X)] = 1\n",
    "    return counts    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "25362568",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AA    0\n",
       "AC    1\n",
       "AG    0\n",
       "AT    0\n",
       "CA    0\n",
       "CC    0\n",
       "CG    1\n",
       "CT    0\n",
       "GA    0\n",
       "GC    0\n",
       "GG    0\n",
       "GT    1\n",
       "TA    0\n",
       "TC    0\n",
       "TG    0\n",
       "TT    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# CODE PROVIDED -- DO NOT REMOVE\n",
    "# X has 3 words of size 2 (AC, CG, GT)\n",
    "X = \"ACGT\"\n",
    "get_kmer_2(\"ACGT\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9cf6056",
   "metadata": {},
   "source": [
    "The function below takes a dictionary of sequences' counts as a `pandas Series` and prints it using HTML Table, which you might agree is nicer to visualize."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "96eab270",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CODE PROVIDED -- DO NOT REMOVE\n",
    "def pretty_print_counts(counts_dict):\n",
    "    list_of_count = [data.to_list() for data in counts_dict.values()]\n",
    "    list_of_indices = [x for x in counts_dict.keys()]\n",
    "    list_of_columns = list(counts_dict.values())[0].index.to_list()\n",
    "    df_single_level_cols = pd.DataFrame(list_of_count,\n",
    "                                        index=[x for x in counts_dict.keys()],\n",
    "                                       columns = list_of_columns)    \n",
    "    return df_single_level_cols \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dabf1943",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AA</th>\n",
       "      <th>AC</th>\n",
       "      <th>AG</th>\n",
       "      <th>AT</th>\n",
       "      <th>CA</th>\n",
       "      <th>CC</th>\n",
       "      <th>CG</th>\n",
       "      <th>CT</th>\n",
       "      <th>GA</th>\n",
       "      <th>GC</th>\n",
       "      <th>GG</th>\n",
       "      <th>GT</th>\n",
       "      <th>TA</th>\n",
       "      <th>TC</th>\n",
       "      <th>TG</th>\n",
       "      <th>TT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>X</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Y</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Z</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   AA  AC  AG  AT  CA  CC  CG  CT  GA  GC  GG  GT  TA  TC  TG  TT\n",
       "X   0   1   0   0   0   0   1   0   0   0   0   1   1   0   0   0\n",
       "Y   1   1   0   0   1   0   1   0   0   0   0   1   1   0   0   0\n",
       "Z   1   1   0   0   1   0   1   0   0   0   0   1   1   0   0   1"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# CODE PROVIDED -- DO NOT REMOVE\n",
    "\n",
    "X = \"ACGTACGTACGTACGT\"\n",
    "Y = \"ACGTACAAACGTACGT\"\n",
    "Z = \"TTTTACAAACGTTTTT\"\n",
    "\n",
    "counts_dict = {\"X\": get_kmer_2(X), \"Y\": get_kmer_2(Y), \"Z\": get_kmer_2(Z)}\n",
    "pretty_print_counts(counts_dict)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f6fad1f",
   "metadata": {},
   "source": [
    "### Q.6\n",
    "\n",
    "Write a function that computes the Jaccard similarity between two feature vectors, A and B. If you recall, Jaccard similarity is computed as:\n",
    "\n",
    "$$J(A,B) = \\frac{A \\cap B}{A \\cup B}$$\n",
    "\n",
    "In other words, the number of items shared by `A` and `B` over the set of all items in `A` or `B`.\n",
    "\n",
    "For example, for `A= get_kmer_2(X)` and Y = get_kmer_2(B) above,\n",
    "\n",
    "$$\n",
    "J(A,B) = \\frac{4}{6}\n",
    "$$\n",
    "\n",
    "Your function should have the following signature: \n",
    "\n",
    "`jaccard(A, B)`\n",
    "\n",
    "Where `A` and `B` are `pandas Series`\n",
    "\n",
    "\n",
    "Test your function using the code below to make sure it's correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "49fd6daf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def jaccard(A, B):\n",
    "    top, bottom = 0, 0\n",
    "    for i in A.keys():\n",
    "        count = A[i] + B[i]\n",
    "        if count > 0:\n",
    "            bottom += 1\n",
    "            if count > 1:\n",
    "                top += 1\n",
    "    return top / bottom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f3303fd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEST PROVIDED -- DO NOT REMOVE\n",
    "A = get_kmer_2(X)\n",
    "B = get_kmer_2(Y)\n",
    "assert jaccard(A, B) == 4/6"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f11b91a0",
   "metadata": {},
   "source": [
    "### Q.7 \n",
    "\n",
    "* Compute the jaccard similarity for the pairs of sequences `(X, Y)`, `(X, Z)`, `(Y, Z)`. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d4923080",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(X, Y) -> 0.6666666666666666\n",
      "(X, Z) -> 0.5714285714285714\n",
      "(Y, Z) -> 0.8571428571428571\n"
     ]
    }
   ],
   "source": [
    "A, B, C = get_kmer_2(X), get_kmer_2(Y), get_kmer_2(Z)\n",
    "print(f'(X, Y) -> {jaccard(A, B)}\\n(X, Z) -> {jaccard(A, C)}\\n(Y, Z) -> {jaccard(B, C)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8695957",
   "metadata": {},
   "source": [
    "### Q.8\n",
    "\n",
    "The vectors representing the presence and absence of words in both `Y` and `Z` are very similar (Jaccard = 0.85), despite major differences at the DNA level between these two sequences. This is because the words are small -- it is as if you were comparing a history book with a book on Python using words of size 2. It's very likely that both books will contain the same words of size 2. Increasing the size of `k` will produce substantial differences. \n",
    "\n",
    "Change the function `get_kmer_2` so that given a sequence `X` and a k-mer size `k`, the function returns a boolean vector of all the words of size `k` in `X`. Cal the function `get_kmer`\n",
    "\n",
    "\n",
    "\n",
    "The following code can be used to generate all DNA words of size `k`\n",
    "```pyton\n",
    "words_size_k = [\"\".join(prod) for prod in product(*([DNA]*k))]\n",
    "```\n",
    "\n",
    "Once done, use the code below to test your function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ec8750a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_kmer(X, k = 2):\n",
    "    DNA = [\"A\", \"C\", \"G\", \"T\"]\n",
    "    words_size_k = [\"\".join(prod) for prod in product(*([DNA]*k))]\n",
    "    counts = pd.Series([0 for _ in words_size_k], index = words_size_k)\n",
    "    words_in_X = set([X[i:i+k] for i in range(len(X)-k+1)])\n",
    "    counts[list(words_in_X)] = 1\n",
    "    return counts    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "072c31fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEST PROVIDED -- DO NOT REMOVE\n",
    "X = \"ACGTGATGATTG\"\n",
    "\n",
    "counts = get_kmer(X, k=1)\n",
    "assert counts.tolist() == [1,1,1,1]\n",
    "\n",
    "\n",
    "counts = get_kmer(X, k=3)\n",
    "assert (counts[[\"ACG\", \"CGT\", \"GTG\", \"TGA\", \"GAT\", \"ATG\", \"ATT\", \"TTG\"]] == 1).sum()  == 8\n",
    "assert (counts.drop([\"ACG\", \"CGT\", \"GTG\", \"TGA\", \"GAT\", \"ATG\", \"ATT\", \"TTG\"]) == 0).sum()  == 56\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a7a056b",
   "metadata": {},
   "source": [
    "### Q.9\n",
    "\n",
    "* Compute the Jaccard similarity for the pairs `(X, Y)`, `(X, Z)`, `(Y, Z)` using `k= 5`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "925d98f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(X, Y) -> 0.4\n",
      "(X, Z) -> 0.0\n",
      "(Y, Z) -> 0.29411764705882354\n"
     ]
    }
   ],
   "source": [
    "X = \"ACGTACGTACGTACGT\"\n",
    "Y = \"ACGTACAAACGTACGT\"\n",
    "Z = \"TTTTACAAACGTTTTT\"\n",
    "\n",
    "A, B, C = get_kmer(X, 5), get_kmer(Y, 5), get_kmer(Z, 5)\n",
    "print(f'(X, Y) -> {jaccard(A, B)}\\n(X, Z) -> {jaccard(A, C)}\\n(Y, Z) -> {jaccard(B, C)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "841829e0",
   "metadata": {},
   "source": [
    "The appropriate word size varies with the length of the text, with larger words depicting similarity more accurately. However, large values of `k` are:\n",
    "1. More computationally intensive to compute. With k = 12, there are $4^12 = 16,777,216$ words to compute for each sequence.\n",
    "\n",
    "2. More likely to skew the distance between fairly similar sequences. For example `k=8`, the Jaccard index between `X` and `Y` is `0`, even though `X` and `Y` have only two mismatching characters. While this is an extreme case due to the fact that X and Y are short, the logic applies to longer sequences and larger values of `k`\n",
    "\n",
    "\n",
    "![](https://www.dropbox.com/s/rhw5szbiohsqu7w/mismatches.png?dl=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "aa64ea97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(X, Y) -> 0.08333333333333333\n"
     ]
    }
   ],
   "source": [
    "A, B = get_kmer(X, 8), get_kmer(Y, 8)\n",
    "print(f'(X, Y) -> {jaccard(A, B)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca7dd5bc",
   "metadata": {},
   "source": [
    "The code I used is provided as a reference below. The code took 7 hours to complete on a single machine and approximately 12 minutes on a larger server with 72 cores and 1TB of RAM. To parallelize the execution, I split the file into files that contain 1000 sequences each and used GNU Parallel to run each file on a single CPU core."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f943c170",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CODE PROVIDED FOR ILLUTRATION -- DO NOT REMOVE\n",
    "# RUNNING LOCALLY MAY TAKE A LONG TIME TO COMPLETE\n",
    "\n",
    "# k = 8 \n",
    "# DNA = [\"A\", \"C\", \"G\", \"T\"]\n",
    "# words_size_k = [\"\".join(prod) for prod in product(*([DNA]*k))]\n",
    "\n",
    "    \n",
    "# def get_kmer_mod(X):\n",
    "#     counts = pd.Series([0 for _ in words_size_k], index = words_size_k)\n",
    "#     words_in_X = set([X[i:i+k] for i in range(0, len(X)-k+1)])\n",
    "#     counts[list(words_in_X)] = 1\n",
    "#     return counts   \n",
    "\n",
    "# def replace_bad_nucs(seq):\n",
    "#     for character in ['W', 'K', \"Y\", \"M\", 'H']:\n",
    "#         seq = seq.replace(character, 'A') \n",
    "        \n",
    "#     for character in ['R', 'S', 'D', \"V\", \"B\"]:\n",
    "#         seq = seq.replace(character, 'C') \n",
    "        \n",
    "#     seq = seq.replace(\"N\", '') \n",
    "    \n",
    "#     return seq\n",
    "\n",
    "# all_counts = []\n",
    "# all_names = []\n",
    "# with tqdm(total=1000) as pbar:\n",
    "#     for record in SeqIO.parse(\"myseq0.fa\", 'fasta'):\n",
    "#         all_names.append(record.id)\n",
    "#         seq = replace_bad_nucs(str(record.seq))\n",
    "\n",
    "#         counts = get_kmer_mod(seq)\n",
    "#         all_counts.append(counts)\n",
    "#         pbar.update(1)\n",
    "    \n",
    "# kmer_counts = pd.DataFrame(all_counts, index = all_names)\n",
    "# kmer_counts.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cc55a33",
   "metadata": {},
   "source": [
    "### Hashing Sequences\n",
    "\n",
    "We are interested in finding pairs of sequences that are very similar. However, comparing the sequences pairwise is not tractable since it would require carrying out $429282 * (429282 - 1) / 2 = 92_141_303_121$ comparisons.\n",
    "\n",
    "Instead, we will use the hashing-based approach covered in class. Rather than hashing a sequence over all k-mers, we will only compute the hash for a subset of k-mers. we will repeat the operation n times to avoid that similar sequences are assigned to different bins due to a single, rare mismatch.\n",
    "\n",
    "This, as discussed in class, is computationally more efficient compared to computing all pairwise sequences. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab434db6",
   "metadata": {},
   "source": [
    "### Q.10 \n",
    "\n",
    "Write a function that takes a `pandas  Series` and a subset of columns and returns the hash computed on the subset of columns. Call this function`hash_on_subset`.\n",
    "\n",
    "As an example, consider all words with a size of 2 as follows \n",
    "\n",
    "|\t|AA\t|AC\t|AG\t|AT\t|CA | CC| CG| CT| GA| GC| GG| GT| TA| TC| TG| TT|\n",
    "|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|\n",
    "| A\t|0\t|1\t|0\t|0\t|0\t|0\t|1\t|0\t| 0 |0\t|0\t| 1 |1  |0\t|0\t|0  |\n",
    "\n",
    "```python\n",
    "hash_on_subset(A, [\"AC\", \"CG\", \"CT\", \"GT\", \"TA\"]) \n",
    "```\n",
    "\n",
    "is equivalent to:\n",
    "\n",
    "```python\n",
    "hash((1, 1, 0, 1, 1)) == 5085477689562523216\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "baefa80a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5085477689562523216"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def hash_on_subset(pan, arr):\n",
    "    res = []\n",
    "    for i in arr:\n",
    "        res.append(pan[i])\n",
    "    return hash(tuple(res))\n",
    "\n",
    "A = get_kmer_2(X)\n",
    "hash_on_subset(A, [\"AC\", \"CG\", \"CT\", \"GT\", \"TA\"]) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "982ff5a6",
   "metadata": {},
   "source": [
    "\n",
    "The method `sample` from the random module `m` words from a list\n",
    "\n",
    "For example, running:\n",
    "```python\n",
    "random.sample( [\"A\", \"C\", \"G\", \"T\"], 2 )\n",
    "```\n",
    "returns\n",
    "```\n",
    "['A', 'C']\n",
    "```\n",
    "The returned subset may be different for you.\n",
    "\n",
    "* The code below randomly selects `m=20` k-mers we will use to compare the genomes\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c471e03c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['T', 'A']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random.sample( [\"A\", \"C\", \"G\", \"T\"], 2 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9c6c8245",
   "metadata": {},
   "outputs": [],
   "source": [
    "k=8\n",
    "DNA = [\"A\", \"C\", \"G\", \"T\"]\n",
    "words_size_k = [\"\".join(prod) for prod in product(*([DNA]*k))]\n",
    "\n",
    "m=20\n",
    "subset_kmers = random.sample(words_size_k, m)\n",
    "\n",
    "# subset_kmers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb434317",
   "metadata": {},
   "source": [
    "### Q.12\n",
    "\n",
    "\n",
    "Apply the function `hash_subset` to all the rows of `all_kmers_df`. The data science (*vectorized*) way to do so is using the `apply` method available on a `pandas DataFrame` instead of using for loops. For example, given a DataFrame `df` such that:\n",
    "\n",
    "```\n",
    "df = pd.DataFrame([[1,2,3], [4,5,6]])\n",
    "\n",
    "```\n",
    "then \n",
    "```\n",
    "df.apply(max, args=[] axis=1)\n",
    "```\n",
    "applies the `max()` function on each row (`axis = 1`). Here, `args` is empty since `max` does not take any additional arguments.\n",
    "\n",
    "The example below shows how to use `apply` when the function requires additional arguments. In this example, we apply a function that sums all the values of a row and adds to the sum an offset (2 by default)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "41b99f78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The sum of rows + an offset of 5 is:\n",
      "0    11\n",
      "1    20\n",
      "dtype: int64\n",
      "The sum of rows + an offset of 10 is:\n",
      "0    16\n",
      "1    25\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# EXAMPLE CODE PROVIDED -- DO NOT REMOVE\n",
    "def add_val_to_sum(x, offset=2):\n",
    "    return x.sum() + offset\n",
    "    \n",
    "df = pd.DataFrame([[1,2,3], [4,5,6]])\n",
    "\n",
    "print(\"The sum of rows + an offset of 5 is:\")\n",
    "print(df.apply(add_val_to_sum, args=[5], axis=1))\n",
    "\n",
    "print(\"The sum of rows + an offset of 10 is:\")\n",
    "print(df.apply(add_val_to_sum, args=[10], axis=1))\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b327b450",
   "metadata": {},
   "source": [
    "### Q.13\n",
    "\n",
    "\n",
    "Use `apply()` to apply `hash_subset` and compute the hash values for all the rows of `all_kmers_df` over `subset_kmers`\n",
    "\n",
    "* Create a dict by parsing the results to group sequences that yield the same hash under the same bins. Each key in the dict should be a key and each value is a list of sequences that have the same value.\n",
    "\n",
    "For example, in the dictionary below, X and Y have the same hash value (123456) over a given subset of kmers, whereas Z has a different hash over the same subsets.\n",
    "\n",
    "```\n",
    "{\"123456\": [X,Y], \"654321\": [Z]}\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "805bb429",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{-4057562519893178744: [0,\n",
       "  2,\n",
       "  3,\n",
       "  4,\n",
       "  5,\n",
       "  6,\n",
       "  8,\n",
       "  9,\n",
       "  10,\n",
       "  11,\n",
       "  12,\n",
       "  14,\n",
       "  15,\n",
       "  16,\n",
       "  17,\n",
       "  18,\n",
       "  19,\n",
       "  20,\n",
       "  21,\n",
       "  22,\n",
       "  23,\n",
       "  24,\n",
       "  26,\n",
       "  27,\n",
       "  28,\n",
       "  29,\n",
       "  30,\n",
       "  31,\n",
       "  32,\n",
       "  33,\n",
       "  34,\n",
       "  35,\n",
       "  36,\n",
       "  38,\n",
       "  39,\n",
       "  40,\n",
       "  41,\n",
       "  42,\n",
       "  43,\n",
       "  44,\n",
       "  45,\n",
       "  46,\n",
       "  47,\n",
       "  48,\n",
       "  49,\n",
       "  50,\n",
       "  51,\n",
       "  52,\n",
       "  53,\n",
       "  54,\n",
       "  55,\n",
       "  56,\n",
       "  57,\n",
       "  58,\n",
       "  59,\n",
       "  60,\n",
       "  61,\n",
       "  62,\n",
       "  63,\n",
       "  64,\n",
       "  65,\n",
       "  66,\n",
       "  67,\n",
       "  68,\n",
       "  69,\n",
       "  70,\n",
       "  71,\n",
       "  72,\n",
       "  73,\n",
       "  74,\n",
       "  75,\n",
       "  76,\n",
       "  77,\n",
       "  78,\n",
       "  79,\n",
       "  80,\n",
       "  81,\n",
       "  82,\n",
       "  83,\n",
       "  84,\n",
       "  85,\n",
       "  86,\n",
       "  87,\n",
       "  88,\n",
       "  89,\n",
       "  90,\n",
       "  91,\n",
       "  93,\n",
       "  94,\n",
       "  95,\n",
       "  96,\n",
       "  97,\n",
       "  98,\n",
       "  99,\n",
       "  100,\n",
       "  101,\n",
       "  102,\n",
       "  103,\n",
       "  106,\n",
       "  107,\n",
       "  108,\n",
       "  109,\n",
       "  110,\n",
       "  111,\n",
       "  112,\n",
       "  113,\n",
       "  114,\n",
       "  115,\n",
       "  116,\n",
       "  117,\n",
       "  118,\n",
       "  119,\n",
       "  120,\n",
       "  121,\n",
       "  122,\n",
       "  123,\n",
       "  124,\n",
       "  125,\n",
       "  126,\n",
       "  127,\n",
       "  128,\n",
       "  129,\n",
       "  130,\n",
       "  131,\n",
       "  132,\n",
       "  133,\n",
       "  134,\n",
       "  135,\n",
       "  136,\n",
       "  137,\n",
       "  138,\n",
       "  139,\n",
       "  140,\n",
       "  141,\n",
       "  142,\n",
       "  143,\n",
       "  144,\n",
       "  145,\n",
       "  146,\n",
       "  147,\n",
       "  148,\n",
       "  149,\n",
       "  150,\n",
       "  151,\n",
       "  152,\n",
       "  153,\n",
       "  154,\n",
       "  155,\n",
       "  156,\n",
       "  157,\n",
       "  158,\n",
       "  159,\n",
       "  160,\n",
       "  161,\n",
       "  162,\n",
       "  163,\n",
       "  164,\n",
       "  165,\n",
       "  166,\n",
       "  167,\n",
       "  168,\n",
       "  169,\n",
       "  170,\n",
       "  172,\n",
       "  173,\n",
       "  174,\n",
       "  175,\n",
       "  176,\n",
       "  177,\n",
       "  178,\n",
       "  179,\n",
       "  180,\n",
       "  181,\n",
       "  182,\n",
       "  183,\n",
       "  184,\n",
       "  185,\n",
       "  186,\n",
       "  187,\n",
       "  188,\n",
       "  189,\n",
       "  190,\n",
       "  192,\n",
       "  193,\n",
       "  195,\n",
       "  196,\n",
       "  197,\n",
       "  198,\n",
       "  200,\n",
       "  201,\n",
       "  202,\n",
       "  203,\n",
       "  204,\n",
       "  205,\n",
       "  206,\n",
       "  207,\n",
       "  208,\n",
       "  209,\n",
       "  210,\n",
       "  211,\n",
       "  212,\n",
       "  213,\n",
       "  214,\n",
       "  215,\n",
       "  216,\n",
       "  217,\n",
       "  218,\n",
       "  219,\n",
       "  220,\n",
       "  221,\n",
       "  222,\n",
       "  223,\n",
       "  224,\n",
       "  225,\n",
       "  226,\n",
       "  227,\n",
       "  228,\n",
       "  229,\n",
       "  230,\n",
       "  231,\n",
       "  233,\n",
       "  234,\n",
       "  235,\n",
       "  236,\n",
       "  237,\n",
       "  238,\n",
       "  239,\n",
       "  240,\n",
       "  241,\n",
       "  242,\n",
       "  243,\n",
       "  244,\n",
       "  245,\n",
       "  246,\n",
       "  247,\n",
       "  248,\n",
       "  250,\n",
       "  252,\n",
       "  253,\n",
       "  254,\n",
       "  255,\n",
       "  256,\n",
       "  257,\n",
       "  258,\n",
       "  260,\n",
       "  261,\n",
       "  262,\n",
       "  264,\n",
       "  265,\n",
       "  267,\n",
       "  268,\n",
       "  269,\n",
       "  270,\n",
       "  271,\n",
       "  272,\n",
       "  273,\n",
       "  274,\n",
       "  275,\n",
       "  276,\n",
       "  277,\n",
       "  278,\n",
       "  279,\n",
       "  280,\n",
       "  281,\n",
       "  282,\n",
       "  283,\n",
       "  284,\n",
       "  285,\n",
       "  286,\n",
       "  287,\n",
       "  288,\n",
       "  289,\n",
       "  290,\n",
       "  291,\n",
       "  292,\n",
       "  293,\n",
       "  294,\n",
       "  295,\n",
       "  296,\n",
       "  297,\n",
       "  298,\n",
       "  299,\n",
       "  300,\n",
       "  301,\n",
       "  302,\n",
       "  303,\n",
       "  304,\n",
       "  305,\n",
       "  306,\n",
       "  307,\n",
       "  308,\n",
       "  309,\n",
       "  310,\n",
       "  311,\n",
       "  312,\n",
       "  313,\n",
       "  314,\n",
       "  315,\n",
       "  316,\n",
       "  317,\n",
       "  318,\n",
       "  319,\n",
       "  320,\n",
       "  321,\n",
       "  322,\n",
       "  323,\n",
       "  324,\n",
       "  325,\n",
       "  326,\n",
       "  327,\n",
       "  328,\n",
       "  329,\n",
       "  330,\n",
       "  331,\n",
       "  332,\n",
       "  333,\n",
       "  334,\n",
       "  335,\n",
       "  336,\n",
       "  337,\n",
       "  338,\n",
       "  339,\n",
       "  340,\n",
       "  341,\n",
       "  342,\n",
       "  343,\n",
       "  344,\n",
       "  345,\n",
       "  346,\n",
       "  347,\n",
       "  348,\n",
       "  349,\n",
       "  350,\n",
       "  351,\n",
       "  352,\n",
       "  353,\n",
       "  354,\n",
       "  355,\n",
       "  356,\n",
       "  357,\n",
       "  359,\n",
       "  360,\n",
       "  361,\n",
       "  362,\n",
       "  363,\n",
       "  364,\n",
       "  365,\n",
       "  366,\n",
       "  367,\n",
       "  368,\n",
       "  369,\n",
       "  370,\n",
       "  371,\n",
       "  372,\n",
       "  373,\n",
       "  374,\n",
       "  375,\n",
       "  376,\n",
       "  377,\n",
       "  378,\n",
       "  379,\n",
       "  380,\n",
       "  381,\n",
       "  382,\n",
       "  383,\n",
       "  384,\n",
       "  385,\n",
       "  386,\n",
       "  387,\n",
       "  388,\n",
       "  389,\n",
       "  390,\n",
       "  391,\n",
       "  392,\n",
       "  393,\n",
       "  394,\n",
       "  395,\n",
       "  396,\n",
       "  397,\n",
       "  398,\n",
       "  401,\n",
       "  402,\n",
       "  403,\n",
       "  404,\n",
       "  405,\n",
       "  406,\n",
       "  407,\n",
       "  408,\n",
       "  409,\n",
       "  410,\n",
       "  411,\n",
       "  412,\n",
       "  413,\n",
       "  414,\n",
       "  415,\n",
       "  416,\n",
       "  417,\n",
       "  418,\n",
       "  419,\n",
       "  420,\n",
       "  421,\n",
       "  422,\n",
       "  423,\n",
       "  424,\n",
       "  425,\n",
       "  426,\n",
       "  427,\n",
       "  428,\n",
       "  429,\n",
       "  430,\n",
       "  431,\n",
       "  432,\n",
       "  433,\n",
       "  434,\n",
       "  435,\n",
       "  436,\n",
       "  437,\n",
       "  438,\n",
       "  439,\n",
       "  440,\n",
       "  441,\n",
       "  442,\n",
       "  443,\n",
       "  444,\n",
       "  445,\n",
       "  446,\n",
       "  447,\n",
       "  448,\n",
       "  449,\n",
       "  450,\n",
       "  451,\n",
       "  452,\n",
       "  453,\n",
       "  454,\n",
       "  455,\n",
       "  456,\n",
       "  457,\n",
       "  458,\n",
       "  459,\n",
       "  460,\n",
       "  461,\n",
       "  462,\n",
       "  463,\n",
       "  464,\n",
       "  465,\n",
       "  466,\n",
       "  467,\n",
       "  468,\n",
       "  469,\n",
       "  470,\n",
       "  471,\n",
       "  472,\n",
       "  473,\n",
       "  474,\n",
       "  475,\n",
       "  476,\n",
       "  477,\n",
       "  478,\n",
       "  479,\n",
       "  480,\n",
       "  481,\n",
       "  482,\n",
       "  483,\n",
       "  484,\n",
       "  485,\n",
       "  486,\n",
       "  487,\n",
       "  488,\n",
       "  489,\n",
       "  490,\n",
       "  491,\n",
       "  492,\n",
       "  493,\n",
       "  494,\n",
       "  495,\n",
       "  496,\n",
       "  497,\n",
       "  498,\n",
       "  499,\n",
       "  500,\n",
       "  501,\n",
       "  502,\n",
       "  503,\n",
       "  504,\n",
       "  505,\n",
       "  506,\n",
       "  507,\n",
       "  508,\n",
       "  509,\n",
       "  510,\n",
       "  511,\n",
       "  513,\n",
       "  514,\n",
       "  515,\n",
       "  516,\n",
       "  517,\n",
       "  518,\n",
       "  520,\n",
       "  521,\n",
       "  522,\n",
       "  523,\n",
       "  524,\n",
       "  525,\n",
       "  526,\n",
       "  527,\n",
       "  528,\n",
       "  529,\n",
       "  530,\n",
       "  531,\n",
       "  532,\n",
       "  533,\n",
       "  534,\n",
       "  535,\n",
       "  536,\n",
       "  537,\n",
       "  538,\n",
       "  539,\n",
       "  540,\n",
       "  541,\n",
       "  542,\n",
       "  543,\n",
       "  544,\n",
       "  545,\n",
       "  546,\n",
       "  547,\n",
       "  548,\n",
       "  549,\n",
       "  550,\n",
       "  551,\n",
       "  552,\n",
       "  553,\n",
       "  554,\n",
       "  555,\n",
       "  556,\n",
       "  557,\n",
       "  558,\n",
       "  559,\n",
       "  560,\n",
       "  561,\n",
       "  562,\n",
       "  563,\n",
       "  564,\n",
       "  565,\n",
       "  566,\n",
       "  567,\n",
       "  568,\n",
       "  569,\n",
       "  570,\n",
       "  571,\n",
       "  572,\n",
       "  573,\n",
       "  574,\n",
       "  575,\n",
       "  576,\n",
       "  577,\n",
       "  578,\n",
       "  579,\n",
       "  580,\n",
       "  581,\n",
       "  582,\n",
       "  583,\n",
       "  584,\n",
       "  585,\n",
       "  586,\n",
       "  587,\n",
       "  588,\n",
       "  589,\n",
       "  590,\n",
       "  591,\n",
       "  592,\n",
       "  593,\n",
       "  594,\n",
       "  595,\n",
       "  596,\n",
       "  597,\n",
       "  598,\n",
       "  599,\n",
       "  600,\n",
       "  601,\n",
       "  602,\n",
       "  603,\n",
       "  604,\n",
       "  605,\n",
       "  606,\n",
       "  607,\n",
       "  608,\n",
       "  609,\n",
       "  610,\n",
       "  611,\n",
       "  612,\n",
       "  613,\n",
       "  614,\n",
       "  615,\n",
       "  616,\n",
       "  617,\n",
       "  618,\n",
       "  619,\n",
       "  620,\n",
       "  621,\n",
       "  622,\n",
       "  623,\n",
       "  624,\n",
       "  625,\n",
       "  626,\n",
       "  627,\n",
       "  628,\n",
       "  629,\n",
       "  630,\n",
       "  631,\n",
       "  632,\n",
       "  633,\n",
       "  634,\n",
       "  635,\n",
       "  636,\n",
       "  637,\n",
       "  638,\n",
       "  639,\n",
       "  640,\n",
       "  641,\n",
       "  642,\n",
       "  643,\n",
       "  644,\n",
       "  645,\n",
       "  646,\n",
       "  647,\n",
       "  648,\n",
       "  649,\n",
       "  650,\n",
       "  651,\n",
       "  652,\n",
       "  653,\n",
       "  654,\n",
       "  655,\n",
       "  656,\n",
       "  657,\n",
       "  658,\n",
       "  659,\n",
       "  660,\n",
       "  661,\n",
       "  662,\n",
       "  663,\n",
       "  664,\n",
       "  665,\n",
       "  666,\n",
       "  667,\n",
       "  668,\n",
       "  669,\n",
       "  670,\n",
       "  671,\n",
       "  672,\n",
       "  673,\n",
       "  674,\n",
       "  675,\n",
       "  676,\n",
       "  677,\n",
       "  679,\n",
       "  680,\n",
       "  681,\n",
       "  682,\n",
       "  683,\n",
       "  684,\n",
       "  685,\n",
       "  686,\n",
       "  687,\n",
       "  688,\n",
       "  689,\n",
       "  690,\n",
       "  691,\n",
       "  692,\n",
       "  693,\n",
       "  694,\n",
       "  695,\n",
       "  696,\n",
       "  697,\n",
       "  698,\n",
       "  699,\n",
       "  700,\n",
       "  701,\n",
       "  702,\n",
       "  703,\n",
       "  704,\n",
       "  705,\n",
       "  706,\n",
       "  707,\n",
       "  708,\n",
       "  709,\n",
       "  710,\n",
       "  711,\n",
       "  712,\n",
       "  713,\n",
       "  714,\n",
       "  715,\n",
       "  716,\n",
       "  717,\n",
       "  718,\n",
       "  719,\n",
       "  720,\n",
       "  721,\n",
       "  722,\n",
       "  723,\n",
       "  724,\n",
       "  725,\n",
       "  726,\n",
       "  727,\n",
       "  728,\n",
       "  729,\n",
       "  730,\n",
       "  731,\n",
       "  732,\n",
       "  733,\n",
       "  734,\n",
       "  735,\n",
       "  736,\n",
       "  737,\n",
       "  738,\n",
       "  739,\n",
       "  740,\n",
       "  741,\n",
       "  742,\n",
       "  743,\n",
       "  744,\n",
       "  745,\n",
       "  746,\n",
       "  747,\n",
       "  748,\n",
       "  749,\n",
       "  750,\n",
       "  751,\n",
       "  752,\n",
       "  753,\n",
       "  754,\n",
       "  756,\n",
       "  757,\n",
       "  758,\n",
       "  759,\n",
       "  760,\n",
       "  761,\n",
       "  762,\n",
       "  763,\n",
       "  764,\n",
       "  765,\n",
       "  766,\n",
       "  767,\n",
       "  768,\n",
       "  769,\n",
       "  770,\n",
       "  771,\n",
       "  772,\n",
       "  773,\n",
       "  774,\n",
       "  775,\n",
       "  776,\n",
       "  777,\n",
       "  778,\n",
       "  779,\n",
       "  780,\n",
       "  781,\n",
       "  782,\n",
       "  783,\n",
       "  784,\n",
       "  785,\n",
       "  786,\n",
       "  787,\n",
       "  788,\n",
       "  789,\n",
       "  790,\n",
       "  791,\n",
       "  792,\n",
       "  793,\n",
       "  794,\n",
       "  795,\n",
       "  796,\n",
       "  797,\n",
       "  798,\n",
       "  799,\n",
       "  800,\n",
       "  801,\n",
       "  802,\n",
       "  803,\n",
       "  804,\n",
       "  805,\n",
       "  806,\n",
       "  807,\n",
       "  808,\n",
       "  809,\n",
       "  810,\n",
       "  812,\n",
       "  813,\n",
       "  814,\n",
       "  815,\n",
       "  816,\n",
       "  817,\n",
       "  818,\n",
       "  819,\n",
       "  820,\n",
       "  821,\n",
       "  822,\n",
       "  823,\n",
       "  824,\n",
       "  825,\n",
       "  826,\n",
       "  827,\n",
       "  828,\n",
       "  829,\n",
       "  830,\n",
       "  831,\n",
       "  832,\n",
       "  833,\n",
       "  834,\n",
       "  835,\n",
       "  836,\n",
       "  837,\n",
       "  838,\n",
       "  839,\n",
       "  840,\n",
       "  841,\n",
       "  842,\n",
       "  843,\n",
       "  844,\n",
       "  845,\n",
       "  846,\n",
       "  847,\n",
       "  848,\n",
       "  849,\n",
       "  850,\n",
       "  851,\n",
       "  852,\n",
       "  853,\n",
       "  854,\n",
       "  855,\n",
       "  856,\n",
       "  857,\n",
       "  858,\n",
       "  859,\n",
       "  860,\n",
       "  861,\n",
       "  862,\n",
       "  863,\n",
       "  864,\n",
       "  865,\n",
       "  867,\n",
       "  868,\n",
       "  869,\n",
       "  870,\n",
       "  871,\n",
       "  873,\n",
       "  874,\n",
       "  875,\n",
       "  876,\n",
       "  877,\n",
       "  878,\n",
       "  879,\n",
       "  880,\n",
       "  881,\n",
       "  882,\n",
       "  883,\n",
       "  884,\n",
       "  885,\n",
       "  886,\n",
       "  887,\n",
       "  888,\n",
       "  889,\n",
       "  890,\n",
       "  892,\n",
       "  893,\n",
       "  894,\n",
       "  895,\n",
       "  896,\n",
       "  897,\n",
       "  898,\n",
       "  899,\n",
       "  900,\n",
       "  901,\n",
       "  902,\n",
       "  903,\n",
       "  904,\n",
       "  905,\n",
       "  906,\n",
       "  907,\n",
       "  908,\n",
       "  909,\n",
       "  910,\n",
       "  911,\n",
       "  912,\n",
       "  913,\n",
       "  914,\n",
       "  915,\n",
       "  916,\n",
       "  917,\n",
       "  918,\n",
       "  919,\n",
       "  920,\n",
       "  921,\n",
       "  922,\n",
       "  923,\n",
       "  924,\n",
       "  925,\n",
       "  926,\n",
       "  927,\n",
       "  928,\n",
       "  929,\n",
       "  930,\n",
       "  931,\n",
       "  932,\n",
       "  933,\n",
       "  934,\n",
       "  935,\n",
       "  936,\n",
       "  937,\n",
       "  938,\n",
       "  939,\n",
       "  940,\n",
       "  941,\n",
       "  942,\n",
       "  943,\n",
       "  944,\n",
       "  945,\n",
       "  946,\n",
       "  947,\n",
       "  948,\n",
       "  949,\n",
       "  950,\n",
       "  951,\n",
       "  952,\n",
       "  953,\n",
       "  954,\n",
       "  955,\n",
       "  956,\n",
       "  957,\n",
       "  958,\n",
       "  959,\n",
       "  960,\n",
       "  961,\n",
       "  962,\n",
       "  963,\n",
       "  964,\n",
       "  965,\n",
       "  966,\n",
       "  967,\n",
       "  968,\n",
       "  969,\n",
       "  970,\n",
       "  971,\n",
       "  972,\n",
       "  973,\n",
       "  974,\n",
       "  975,\n",
       "  976,\n",
       "  977,\n",
       "  978,\n",
       "  979,\n",
       "  980,\n",
       "  982,\n",
       "  983,\n",
       "  984,\n",
       "  985,\n",
       "  986,\n",
       "  987,\n",
       "  988,\n",
       "  989,\n",
       "  991,\n",
       "  992,\n",
       "  993,\n",
       "  994,\n",
       "  996,\n",
       "  997,\n",
       "  998,\n",
       "  999,\n",
       "  1000,\n",
       "  1001,\n",
       "  1003,\n",
       "  1004,\n",
       "  1005,\n",
       "  1006,\n",
       "  1007,\n",
       "  1008,\n",
       "  1009,\n",
       "  1010,\n",
       "  1011,\n",
       "  1012,\n",
       "  1013,\n",
       "  1015,\n",
       "  1016,\n",
       "  1017,\n",
       "  1018,\n",
       "  1019,\n",
       "  1020,\n",
       "  1021,\n",
       "  1022,\n",
       "  1023,\n",
       "  1024,\n",
       "  1025,\n",
       "  1027,\n",
       "  1028,\n",
       "  1030,\n",
       "  1031,\n",
       "  1032,\n",
       "  1033,\n",
       "  1034,\n",
       "  1035,\n",
       "  ...],\n",
       " -6247903785424933384: [1,\n",
       "  13,\n",
       "  25,\n",
       "  512,\n",
       "  1026,\n",
       "  1291,\n",
       "  1684,\n",
       "  2038,\n",
       "  2156,\n",
       "  2159,\n",
       "  2654,\n",
       "  2698,\n",
       "  2711,\n",
       "  2742,\n",
       "  3019,\n",
       "  3096,\n",
       "  3345,\n",
       "  3696,\n",
       "  3900,\n",
       "  4347,\n",
       "  4759,\n",
       "  4810,\n",
       "  5179,\n",
       "  5510,\n",
       "  6224,\n",
       "  6247,\n",
       "  6290,\n",
       "  6314,\n",
       "  6451,\n",
       "  6660,\n",
       "  7241,\n",
       "  7566,\n",
       "  7787,\n",
       "  7892,\n",
       "  8461,\n",
       "  8522,\n",
       "  8669,\n",
       "  8853,\n",
       "  9038,\n",
       "  9143,\n",
       "  9275,\n",
       "  9581],\n",
       " 2301999240539399922: [7, 3718, 5137, 6051],\n",
       " -581837317721311700: [37,\n",
       "  400,\n",
       "  519,\n",
       "  866,\n",
       "  891,\n",
       "  981,\n",
       "  1792,\n",
       "  1860,\n",
       "  1973,\n",
       "  2117,\n",
       "  2132,\n",
       "  2449,\n",
       "  2594,\n",
       "  3050,\n",
       "  3240,\n",
       "  3341,\n",
       "  3647,\n",
       "  3649,\n",
       "  3732,\n",
       "  4126,\n",
       "  4190,\n",
       "  4312,\n",
       "  4486,\n",
       "  4564,\n",
       "  4610,\n",
       "  5281,\n",
       "  5345,\n",
       "  5370,\n",
       "  6206,\n",
       "  6211,\n",
       "  6237,\n",
       "  6249,\n",
       "  6403,\n",
       "  6954,\n",
       "  7065,\n",
       "  7103,\n",
       "  7142,\n",
       "  7271,\n",
       "  7717,\n",
       "  7748,\n",
       "  7794,\n",
       "  7903,\n",
       "  8593,\n",
       "  8860,\n",
       "  9050,\n",
       "  9060,\n",
       "  9173,\n",
       "  9212,\n",
       "  9639,\n",
       "  9662],\n",
       " -4405497805564603832: [92, 249, 1361, 3485, 4168, 9447],\n",
       " -4066286162057155020: [104,\n",
       "  251,\n",
       "  263,\n",
       "  358,\n",
       "  990,\n",
       "  995,\n",
       "  1002,\n",
       "  1014,\n",
       "  1047,\n",
       "  1821,\n",
       "  1835,\n",
       "  1848,\n",
       "  1850,\n",
       "  1920,\n",
       "  2064,\n",
       "  2221,\n",
       "  2708,\n",
       "  3141,\n",
       "  3932,\n",
       "  4480,\n",
       "  4526,\n",
       "  4557,\n",
       "  4586,\n",
       "  4612,\n",
       "  5170,\n",
       "  5309,\n",
       "  6143,\n",
       "  6215,\n",
       "  6235,\n",
       "  6307,\n",
       "  6311,\n",
       "  6702,\n",
       "  6783,\n",
       "  7004,\n",
       "  7008,\n",
       "  7019,\n",
       "  7027,\n",
       "  7095,\n",
       "  7224,\n",
       "  7296,\n",
       "  7862,\n",
       "  7883,\n",
       "  7954,\n",
       "  8082,\n",
       "  8881,\n",
       "  9601,\n",
       "  9675],\n",
       " -1145407377076709454: [105,\n",
       "  171,\n",
       "  191,\n",
       "  266,\n",
       "  399,\n",
       "  1029,\n",
       "  1036,\n",
       "  1043,\n",
       "  1050,\n",
       "  1062,\n",
       "  1356,\n",
       "  1564,\n",
       "  1690,\n",
       "  1866,\n",
       "  2008,\n",
       "  2472,\n",
       "  2694,\n",
       "  2765,\n",
       "  2815,\n",
       "  2878,\n",
       "  3503,\n",
       "  3540,\n",
       "  3629,\n",
       "  3684,\n",
       "  3698,\n",
       "  4141,\n",
       "  4430,\n",
       "  4693,\n",
       "  5276,\n",
       "  5544,\n",
       "  5902,\n",
       "  6941,\n",
       "  7024,\n",
       "  7846,\n",
       "  7854,\n",
       "  8436,\n",
       "  8682,\n",
       "  8725,\n",
       "  8743,\n",
       "  8854,\n",
       "  9295,\n",
       "  9584,\n",
       "  9637],\n",
       " 4398522914585282392: [194],\n",
       " 3198091444864553970: [199],\n",
       " 4539032901851984820: [232, 872, 1921, 2846, 7076, 9227, 9586, 9653],\n",
       " 7995635938534841252: [259, 3834, 6183, 6875],\n",
       " 3602433469821032097: [678,\n",
       "  1074,\n",
       "  1097,\n",
       "  1408,\n",
       "  2218,\n",
       "  2226,\n",
       "  2480,\n",
       "  2686,\n",
       "  3002,\n",
       "  3355,\n",
       "  4059,\n",
       "  4238,\n",
       "  4749,\n",
       "  4806,\n",
       "  4909,\n",
       "  5116,\n",
       "  6432,\n",
       "  6637,\n",
       "  7279,\n",
       "  7309,\n",
       "  7554,\n",
       "  7667,\n",
       "  7669,\n",
       "  8168,\n",
       "  8535,\n",
       "  9866,\n",
       "  9884],\n",
       " 2256888525371639093: [755],\n",
       " -3497778384797873862: [811, 5579, 5679],\n",
       " 385681487434297335: [1057],\n",
       " 3742539338730941112: [1061, 4554],\n",
       " 2450078236208819681: [1312, 4348, 6437],\n",
       " -3543050298322263684: [1385, 6378, 7174, 9798],\n",
       " 3735959357272026114: [1560],\n",
       " 4177964445716063094: [1753],\n",
       " 8504848763913868875: [1909, 7922],\n",
       " 2284182906216029162: [1911, 6016, 9642],\n",
       " -6107609313233446940: [2580],\n",
       " 1646230375354160676: [2747,\n",
       "  5352,\n",
       "  6116,\n",
       "  7064,\n",
       "  7861,\n",
       "  7923,\n",
       "  7996,\n",
       "  8719,\n",
       "  9087,\n",
       "  9578],\n",
       " -8929270814034218396: [2779, 4465],\n",
       " -6135442727857026020: [3438],\n",
       " 1291650469663536239: [3453, 5342],\n",
       " 9028786129407512314: [3606],\n",
       " -6252847841043462770: [3627],\n",
       " -2293229194253578227: [4709],\n",
       " 1380785181120346059: [5161, 8034],\n",
       " -6278759207548667071: [5324, 8760],\n",
       " -8540814053819019940: [5412],\n",
       " -3386827617106121908: [6029],\n",
       " -4334756923309180958: [6203, 8781, 9622],\n",
       " -1304909905292600685: [6261, 9647],\n",
       " -6636610984092602425: [7042],\n",
       " -2294100130308857208: [7070],\n",
       " 4386311762949697849: [8001],\n",
       " 4204103911845407814: [8788],\n",
       " -5774002140059147183: [8932]}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Takes a while\n",
    "all_kmers_df = pd.read_csv('./data/all_kmers_10k.csv', low_memory=False)\n",
    "result = all_kmers_df.apply(hash_on_subset, args=[subset_kmers], axis=1)\n",
    "result_dict = {}\n",
    "for i in range(len(result)):\n",
    "    try:\n",
    "        result_dict[result[i]].append(i)\n",
    "    except KeyError:\n",
    "        result_dict[result[i]] = [i]\n",
    "\n",
    "result_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aff4715",
   "metadata": {},
   "source": [
    "### Q. 15\n",
    "\n",
    "Here we use the presence and absence of words, i.e., a vector of booleans, to encode a sequence. The problem with this approach is that it considers the sequences to be identical, even if their word counts differ substantially. For example, given the sequence `X`, `Y` and `Z` as follows\n",
    "```\n",
    "X = ATAGATAGATAGATAGATT\n",
    "Y = ATAGATAGATAGATAGATT\n",
    "Z = ATAGATTTTTTTTTTTTTT\n",
    "```\n",
    "With k =2, all three sequences mach on their vector of word presence/absense."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9653e701",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AA</th>\n",
       "      <th>AC</th>\n",
       "      <th>AG</th>\n",
       "      <th>AT</th>\n",
       "      <th>CA</th>\n",
       "      <th>CC</th>\n",
       "      <th>CG</th>\n",
       "      <th>CT</th>\n",
       "      <th>GA</th>\n",
       "      <th>GC</th>\n",
       "      <th>GG</th>\n",
       "      <th>GT</th>\n",
       "      <th>TA</th>\n",
       "      <th>TC</th>\n",
       "      <th>TG</th>\n",
       "      <th>TT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>X</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Y</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Z</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   AA  AC  AG  AT  CA  CC  CG  CT  GA  GC  GG  GT  TA  TC  TG  TT\n",
       "X   0   0   1   1   0   0   0   0   1   0   0   0   1   0   0   1\n",
       "Y   0   0   1   1   0   0   0   0   1   0   0   0   1   0   0   1\n",
       "Z   0   0   1   1   0   0   0   0   1   0   0   0   1   0   0   1"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = \"ATAGATAGATAGATAGATT\"\n",
    "Y = \"ATAGATAGATAGATAGATT\"\n",
    "Z = \"ATAGATTTTTTTTTTTTTT\"\n",
    "\n",
    "counts_dict = {\"X\": get_kmer(X, k=2), \"Y\": get_kmer(Y, k=2), \"Z\": get_kmer(Z, k=2)}\n",
    "pretty_print_counts(counts_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbe44285",
   "metadata": {},
   "source": [
    "* Comparing these sequences based on word counts, X is much more similar to Y than it is to Z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d78d04d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AA</th>\n",
       "      <th>AC</th>\n",
       "      <th>AG</th>\n",
       "      <th>AT</th>\n",
       "      <th>CA</th>\n",
       "      <th>CC</th>\n",
       "      <th>CG</th>\n",
       "      <th>CT</th>\n",
       "      <th>GA</th>\n",
       "      <th>GC</th>\n",
       "      <th>GG</th>\n",
       "      <th>GT</th>\n",
       "      <th>TA</th>\n",
       "      <th>TC</th>\n",
       "      <th>TG</th>\n",
       "      <th>TT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>X</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Y</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Z</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   AA  AC  AG  AT  CA  CC  CG  CT  GA  GC  GG  GT  TA  TC  TG  TT\n",
       "X   0   0   4   5   0   0   0   0   4   0   0   0   4   0   0   1\n",
       "Y   0   0   4   5   0   0   0   0   4   0   0   0   4   0   0   1\n",
       "Z   0   0   1   2   0   0   0   0   1   0   0   0   1   0   0  13"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_kmer_counts(X, k):\n",
    "    DNA = [\"A\", \"C\", \"G\", \"T\"]\n",
    "    words_size_k = [\"\".join(prod) for prod in product(*([DNA]*k))]\n",
    "    counts = pd.Series([0 for _ in words_size_k], index = words_size_k)\n",
    "    counts_words_in_x = Counter([X[i:i+k] for i in range(0, len(X)-k+1)])\n",
    "    counts.update(counts_words_in_x)\n",
    "    return counts    \n",
    "\n",
    "\n",
    "counts_dict = {\"X\": get_kmer_counts(X, k=2), \"Y\": get_kmer_counts(Y, k=2), \"Z\": get_kmer_counts(Z, k=2)}\n",
    "pretty_print_counts(counts_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac280486",
   "metadata": {},
   "source": [
    "* Given an example (vectors) to justify why hashing is not ideal with counts.\n",
    "  * Use any means you think are useful to illustrate your point (e.g.: figure, simulation (yes, please!))\n",
    " \n",
    "* Describe how the random project approach discussed in class can help solve the issue discussed\n",
    "  * Use code to illustrate how random projection works in the following example.\n",
    "    * I.e., provide code to provide an example where `X` and `Y` are assigned to the same bin and `Y` is assigned to a different bin.\n",
    "    * You can choose any vector values as needed \n",
    " \n",
    "```python\n",
    "X = [1,2]\n",
    "Y = [2,2]\n",
    "Z = [5,1]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "eb38ffd5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hash for X is -3550055125485641917, Hash for Y is 1901736143494378007\n",
      "As seen above. X and Y are similar but have totally different hashes due to array of counts instead of booleans\n",
      "\n",
      "D = (2.6666666666666665, 1.6666666666666667)\n",
      "The magnitude of D is: 3.1446603773522015\n",
      "\n",
      "X:\n",
      "\tThe magnitude of the projection  1.9079961840114479\n",
      "\tThe ratio of the projection in terms of D is 0.6067415730337078\n",
      "\tProjection occurs in bin 0\n",
      "\n",
      "Y:\n",
      "\tThe magnitude of the projection  2.7559944880165355\n",
      "\tThe ratio of the projection in terms of D is 0.8764044943820223\n",
      "\tProjection occurs in bin 0\n",
      "\n",
      "Z:\n",
      "\tThe magnitude of the projection  4.769990460028619\n",
      "\tThe ratio of the projection in terms of D is 1.5168539325842691\n",
      "\tProjection occurs in bin 1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### Write your code here\n",
    "vectors = {'X': (1,2), 'Y': (2, 2), 'Z': (5,1)}\n",
    "print(f\"Hash for X is {hash(vectors['X'])}, Hash for Y is {hash(vectors['Y'])}\")\n",
    "print('As seen above. X and Y are similar but have totally different hashes due to array of counts instead of booleans\\n')\n",
    "\n",
    "# Using similar code from the class notes\n",
    "D = ((1+2+5)/3, (2+2+1)/3)\n",
    "\n",
    "amp_D = (D[0]**2 + D[1]**2)**.5\n",
    "print(f'D = {D}')\n",
    "print(f\"The magnitude of D is: {amp_D}\\n\")\n",
    "\n",
    "for k, v in vectors.items():\n",
    "    dot_D = vectors[k][0]*D[0] + vectors[k][1]*D[1]\n",
    "    proj = dot_D / amp_D\n",
    "    print(f'{k}:')\n",
    "    print (f\"\\tThe magnitude of the projection  {proj}\")\n",
    "    print(f\"\\tThe ratio of the projection in terms of D is {proj/amp_D}\")\n",
    "    print(f\"\\tProjection occurs in bin {int(proj/amp_D)}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb552495",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
